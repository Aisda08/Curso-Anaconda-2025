{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02180fd1-9814-44ff-96f9-8d444a7ea880",
   "metadata": {},
   "source": [
    "# Gradiente descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb2263-f678-4911-b59c-57095705de20",
   "metadata": {},
   "source": [
    "Agora que temos uma compreensão fundamental de derivadas, podemos aplicá-la a um algoritmo amplamente utilizado em otimização e aprendizado de máquina. **O gradiente descendente** nos permite minimizar ou maximizar uma função usando um processo iterativo aproveitando a inclinação de cada variável. Normalmente, no aprendizado de máquina, tentamos minimizar uma função de perda ou maximizar a verossimilhança. Começaremos com um exemplo trivial e, em seguida, o aplicaremos à regressão linear. Também falaremos sobre gradiente descendente estocástico e o que esperar de modelos mais complexos, como o aprendizado profundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea07a9-7a09-44ac-b010-dc78d2b03dff",
   "metadata": {},
   "source": [
    "## Compreendendo a Descida do Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d2f1d-e89c-4acc-b711-558740fc8dde",
   "metadata": {},
   "source": [
    "Imagine que você está em uma cadeia de montanhas à noite, com apenas uma lanterna. Você sabe que, para voltar à cidade em segurança, precisa chegar ao vale, o ponto mais baixo da cadeia de montanhas. Você usa a lanterna para observar a encosta ao seu redor em todas as direções e pisa na direção que mais desce. Você dá passos maiores para encostas maiores e passos menores para encostas menores. Eventualmente, você chegará a um ponto mais baixo na cadeia de montanhas **onde a inclinação é 0**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39d0d4-fd54-413e-91a1-074d10d6b101",
   "metadata": {},
   "source": [
    "![img](data:image/svg+xml;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c308ab-ae91-49b1-821f-1d03a3d4db7a",
   "metadata": {},
   "source": [
    "Agora, considere um caso em que há apenas um vale, ou um mínimo, em toda a cadeia de montanhas. Chamamos isso de problema **convexo** porque há apenas um mínimo. Problemas convexos incluem regressão linear, regressão logística e problemas de programação linear. Estes são bastante simples de resolver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfa002-31cf-4e54-8aae-1dff98d01e7e",
   "metadata": {},
   "source": [
    "![img](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   width="94.032326mm"
   height="72.158356mm"
   viewBox="0 0 94.032323 72.158355"
   version="1.1"
   id="svg1"
   inkscape:version="1.3 (0e150ed, 2023-07-21)"
   sodipodi:docname="gradient_night.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview1"
     pagecolor="#ffffff"
     bordercolor="#000000"
     borderopacity="0.25"
     inkscape:showpageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:deskcolor="#d1d1d1"
     inkscape:document-units="mm"
     showguides="false"
     inkscape:zoom="1.5519601"
     inkscape:cx="194.59264"
     inkscape:cy="92.785893"
     inkscape:window-width="1472"
     inkscape:window-height="891"
     inkscape:window-x="0"
     inkscape:window-y="37"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs1" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-42.208559,-7.3650704)">
    <rect
       style="fill:#000000;stroke:#000000;stroke-width:0.0867177;stroke-linecap:round;stroke-linejoin:round"
       id="rect3"
       width="93.963081"
       height="72.062286"
       x="42.277805"
       y="7.4083939" />
    <path
       id="path4"
       style="fill:#ffff00;stroke:#000000;stroke-width:0.1;stroke-linecap:round;stroke-linejoin:round"
       d="m 117.79602,15.627192 a 6.1735773,6.1735773 0 0 0 -6.17379,6.173784 6.1735773,6.1735773 0 0 0 6.17379,6.173266 6.1735773,6.1735773 0 0 0 2.85305,-0.715202 5.443975,6.1985774 0 0 1 -2.85305,-5.43326 5.443975,6.1985774 0 0 1 2.91041,-5.469433 6.1735773,6.1735773 0 0 0 -2.91041,-0.729155 z" />
    <g
       id="g2"
       transform="matrix(0.43100203,0,0,0.43301315,24.893057,12.796755)">
      <path
         style="fill:#999999;stroke:#808080;stroke-width:0.100001;stroke-linecap:round;stroke-linejoin:round;opacity:1"
         d="m 40.245732,79.469697 -0.07074,-36.479151 c 0,0 0.06745,-45.1348978 0.110772,-47.6977438 0.101036,-5.9767542 13.150995,9.516355 13.150995,9.516355 0,0 10.999227,10.9067728 13.349053,7.9586058 4.482583,-5.6239912 11.563114,8.357677 13.359094,12.948283 1.79598,4.590607 11.767148,26.133435 18.184302,25.91751 6.671862,-0.224495 22.674242,7.632786 22.674242,10.775697 0,3.142911 13.24247,19.725527 22.67136,19.501033 9.4289,-0.224494 -103.429075,-2.440589 -103.429075,-2.440589 z"
         id="path3"
         sodipodi:nodetypes="ccscssssscc" />
      <g
         id="g1">
        <ellipse
           style="fill:#ffffff;stroke:#ffffff;stroke-width:0.1;stroke-linecap:round;stroke-linejoin:round"
           id="path5"
           cx="106.29778"
           cy="36.592476"
           rx="2.8061717"
           ry="2.9184184" />
        <path
           style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 106.29778,39.510893 v 9.877724"
           id="path6" />
        <path
           style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 106.29778,39.510893 5.67432,4.93886"
           id="path7" />
        <path
           style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 106.29778,49.388617 2.83716,6.510319"
           id="path8" />
        <path
           style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 106.29778,49.388617 -2.85617,6.860317"
           id="path9" />
        <path
           style="fill:#666666;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 106.29778,39.510893 -3.20617,6.959306"
           id="path10" />
        <path
           style="fill:#0000ff;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 101.69615,45.467938 2.99854,1.83493 -1.05994,1.735749 -2.61393,-2.667643 z"
           id="path11"
           sodipodi:nodetypes="ccccc" />
        <path
           style="opacity:0.633423;fill:#ffff00;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 103.63475,49.038615 8.68735,11.164642 8.68001,2.205995 -16.30742,-15.106385 z"
           id="path12" />
      </g>
    </g>
    <path
       style="opacity:1;fill:#999999;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       d="m 83.32914,47.208216 6.976107,2.11354 c 0,0 4.531908,1.023333 5.262859,3.947143 0.730953,2.923813 5.116664,7.894291 7.748094,10.233338 2.63144,2.339049 6.44903,4.385717 9.64858,4.093337 3.19955,-0.292381 13.30334,-3.800954 13.74191,-6.870957 0.43857,-3.07 2.48524,-10.215327 3.21619,-17.285081 0.73096,-7.069754 0.11439,-18.531601 3.2162,-24.086841 3.1018,-5.555241 3.1018,-9.210004 3.1018,-9.210004 l 1e-5,69.327988 -94.032331,0.05275 0.03049,-32.31525 z"
       id="path2"
       sodipodi:nodetypes="ccssssssccccc" />
    <g
       id="g16">
      <rect
         style="opacity:1;fill:#800000;stroke:#800000;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
    <g
       id="g16-3"
       transform="translate(6.6523361,-1.7371174)">
      <rect
         style="opacity:1;fill:#784421;stroke:#784421;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2-9"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13-8" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13-6"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14-5"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15-7"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16-6"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
    <g
       id="g16-9"
       transform="translate(-7.2778015,-0.34661447)">
      <rect
         style="opacity:1;fill:#800000;stroke:#800000;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2-99"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13-1" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13-7"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14-2"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15-3"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16-65"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
  </g>
</svg>
)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14eb2c7-3ecd-4749-93a5-e16d87a63179",
   "metadata": {},
   "source": [
    "Agora, considere uma paisagem com vários **mínimos locais**, ou múltiplos vales nos quais podemos ficar presos. Como esses vales não revelam uma inclinação que desça mais, é fácil para a descida do gradiente ficar presa neles. Chamamos esses tipos de problemas de **problemas não convexos**, e eles são muito mais difíceis de resolver. Problemas não convexos incluem redes neurais e aprendizado profundo. Normalmente, a descida do gradiente estocástico e outras técnicas aleatórias são usadas para lidar com problemas não convexos, e falaremos sobre isso mais tarde."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdaeee-903a-4329-836f-2f274cadd2e8",
   "metadata": {},
   "source": [
    "![img](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   width="94.032326mm"
   height="72.158356mm"
   viewBox="0 0 94.032323 72.158355"
   version="1.1"
   id="svg1"
   inkscape:version="1.3 (0e150ed, 2023-07-21)"
   sodipodi:docname="gradient_night.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview1"
     pagecolor="#ffffff"
     bordercolor="#000000"
     borderopacity="0.25"
     inkscape:showpageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:deskcolor="#d1d1d1"
     inkscape:document-units="mm"
     showguides="false"
     inkscape:zoom="1.8819002"
     inkscape:cx="169.50952"
     inkscape:cy="133.37583"
     inkscape:window-width="1472"
     inkscape:window-height="891"
     inkscape:window-x="0"
     inkscape:window-y="37"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs1" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-42.208559,-7.3650704)">
    <rect
       style="fill:#000000;stroke:#000000;stroke-width:0.0867177;stroke-linecap:round;stroke-linejoin:round"
       id="rect3"
       width="93.963081"
       height="72.062286"
       x="42.277805"
       y="7.4083939" />
    <path
       id="path4"
       style="fill:#ffff00;stroke:#000000;stroke-width:0.1;stroke-linecap:round;stroke-linejoin:round"
       d="m 117.79602,15.627192 a 6.1735773,6.1735773 0 0 0 -6.17379,6.173784 6.1735773,6.1735773 0 0 0 6.17379,6.173266 6.1735773,6.1735773 0 0 0 2.85305,-0.715202 5.443975,6.1985774 0 0 1 -2.85305,-5.43326 5.443975,6.1985774 0 0 1 2.91041,-5.469433 6.1735773,6.1735773 0 0 0 -2.91041,-0.729155 z" />
    <path
       style="opacity:1;fill:#999999;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       d="m 42.208559,23.731471 c 0,0 8.524167,-0.300519 10.910934,3.976665 2.386766,4.277185 3.198929,11.022071 10.399483,10.066846 7.200553,-0.955225 10.941656,0 10.941656,0 17.737548,18.433196 21.245939,13.346498 26.235888,-6.541264 0.2645,-1.348792 4.24105,-4.63774 5.78426,-2.55823 2.39776,3.231046 0.82071,18.596231 8.03034,23.867668 7.20964,5.271433 16.86022,1.704832 16.86022,1.704832 0,0 2.55725,-4.008639 1.70484,-16.473006 -0.85242,-12.464368 3.1647,-20.010428 3.1647,-20.010428 l 1e-5,61.706125 -94.032331,0.05275 z"
       id="path16"
       sodipodi:nodetypes="csscssscscccc" />
    <g
       id="g1"
       transform="matrix(0.43100203,0,0,0.43301315,22.308478,10.750955)">
      <ellipse
         style="fill:#ffffff;stroke:#ffffff;stroke-width:0.1;stroke-linecap:round;stroke-linejoin:round"
         id="path5"
         cx="106.29778"
         cy="36.592476"
         rx="2.8061717"
         ry="2.9184184" />
      <path
         style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 106.29778,39.510893 v 9.877724"
         id="path6" />
      <path
         style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 106.29778,39.510893 5.67432,4.93886"
         id="path7" />
      <path
         style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 106.29778,49.388617 2.83716,6.510319"
         id="path8" />
      <path
         style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 106.29778,49.388617 -2.85617,6.860317"
         id="path9" />
      <path
         style="fill:#666666;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 106.29778,39.510893 -3.20617,6.959306"
         id="path10" />
      <path
         style="fill:#0000ff;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 101.69615,45.467938 2.99854,1.83493 -1.05994,1.735749 -2.61393,-2.667643 z"
         id="path11"
         sodipodi:nodetypes="ccccc" />
      <path
         style="opacity:0.633423;fill:#ffff00;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 103.63475,49.038615 8.68735,12.420578 8.68001,0.950059 -16.30742,-15.106385 z"
         id="path12"
         sodipodi:nodetypes="ccccc" />
    </g>
    <g
       id="g16"
       transform="translate(9.5738528,-12.2748)">
      <rect
         style="opacity:1;fill:#800000;stroke:#800000;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
    <g
       id="g16-3"
       transform="translate(16.226189,-14.011918)">
      <rect
         style="opacity:1;fill:#784421;stroke:#784421;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2-9"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13-8" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13-6"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14-5"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15-7"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16-6"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
    <g
       id="g16-9"
       transform="translate(2.2960513,-12.621415)">
      <rect
         style="opacity:1;fill:#800000;stroke:#800000;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2-99"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13-1" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13-7"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14-2"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15-3"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16-65"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
  </g>
</svg>
)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d22d58-c54a-4309-a05a-b3884393a926",
   "metadata": {},
   "source": [
    "Metaforicamente, a paisagem é uma função matemática para a qual estamos tentando encontrar o ponto mais baixo.\n",
    "\n",
    "Vamos aplicar a descida de gradiente a um problema simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8867e4-2a98-493e-a5fd-46c3c3eba7ec",
   "metadata": {},
   "source": [
    "## Exemplo simples de descida de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70441c31-bd98-4812-b1c5-ceef4e293ae6",
   "metadata": {},
   "source": [
    "Vamos pegar esta função:\n",
    "\n",
    "$\n",
    "f(x) = 3 \\left(x + 1\\right)^{2} + 1\n",
    "$ \n",
    "\n",
    "Aqui está plotado no SymPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd0820f-687d-438b-b62c-c6abd33f8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "\n",
    "x = symbols('x')\n",
    "f = 3*(x+1)**2 + 1\n",
    "plot(f, xlim=(-3,1), ylim=(-1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024ed35-2fd5-4a2f-a4a6-7047b6512244",
   "metadata": {},
   "source": [
    "Poderíamos resolver isso algebricamente, primeiro calculando a derivada da função em relação a $ x $ e, então, resolvendo para onde a inclinação é $ 0 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f5057-eac9-41ff-ab40-d98ad3d9368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.solvers import solve \n",
    "\n",
    "x = symbols('x')\n",
    "f = 3*(x+1)**2 + 1\n",
    "dx = diff(f, x)\n",
    "\n",
    "solve(dx, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68946fc-069b-44e7-b4ed-9fab206b1e55",
   "metadata": {},
   "source": [
    "Embora tenhamos um atalho para resolver este problema simples, não temos atalhos para problemas de aprendizado de máquina mais complexos, nos quais precisamos usar a descida do gradiente. Mas podemos entender a descida do gradiente primeiro aplicando-a a um problema simples como este.\n",
    "\n",
    "Ainda usaremos a derivada da função, mas iniciaremos um algoritmo de busca em um local aleatório para $ x $ que esteja razoavelmente próximo da solução. Abaixo, inicializaremos um $ x $ aleatório, mas o manteremos no intervalo do nosso gráfico acima para consistência visual. Traçaremos a reta tangente para esse local inicial $ x $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c872e2-7e53-4519-8639-ccd41d52255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "# iniciar x em local aleatório\n",
    "x_i = random.uniform(-3,1)\n",
    "\n",
    "# calcular declive em x aleatório\n",
    "# e reta tangente\n",
    "m = dx.subs(x, x_i) \n",
    "b = -(m * x_i - f.subs(x, x_i))\n",
    "\n",
    "plot(f, m*x+b, xlim=(-3,1), ylim=(-1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfbbad-1ab7-422e-a177-24dbcdfef828",
   "metadata": {},
   "source": [
    "Vamos declarar uma **taxa de aprendizado** de $ 0.05 $, que pega uma fração da inclinação e a subtrai do nosso valor x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deddb2e9-5f5e-44c7-9147-0473009d50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = .0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339ba45-d4f7-4ab6-942d-7fecfb691c6b",
   "metadata": {},
   "source": [
    "**Agora execute o bloco de código abaixo várias vezes e observe o que acontece**. Preste atenção ao valor $ x $ e à reta tangente. Para onde ela está convergindo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454317c6-1d56-4e2c-a930-facf522960b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute esta célula de código repetidamente\n",
    "x_i -= m * L \n",
    "m = dx.subs(x, x_i) \n",
    "b = -(m * x_i - f.subs(x, x_i))\n",
    "print(f\"x = {x_i}\")\n",
    "plot(f, m*x+b, xlim=(-3,1), ylim=(-1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a72e2-805f-4e05-bb18-b842fc07061a",
   "metadata": {},
   "source": [
    "Essa busca por um $ x $ que nos dá uma inclinação de $ 0 $ na reta tangente, e portanto o mínimo, é o que chamamos de gradiente descendente. Vamos reempacotar todo o código acima em um loop `for` que faz isso 1000 vezes. Você verá que a reta converge para o mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197734c-cf15-4592-bca1-f9923e68b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.solvers import solve \n",
    "import random \n",
    "\n",
    "# declarar função e derivada\n",
    "x = symbols('x')\n",
    "f = 3*(x+1)**2 + 1\n",
    "dx = diff(f, x) \n",
    "\n",
    "# declarar taxa de aprendizagem\n",
    "L = .01\n",
    "\n",
    "# iniciar x em local aleatório\n",
    "x_i = random.uniform(-3,1)\n",
    "\n",
    "for i in range(1000):\n",
    "    x_i -= m * L \n",
    "    m = dx.subs(x, x_i) \n",
    "    b = -(m * x_i - f.subs(x, x_i))\n",
    "\n",
    "print(f\"x = {x_i}\")\n",
    "plot(f, m*x+b, xlim=(-3,1), ylim=(-1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2af46e-a081-439e-afef-52f4307dbf84",
   "metadata": {},
   "source": [
    "Portanto, no final, a descida do gradiente começa em um local aleatório em nossa função (um $ x $ aleatório) e subtrai repetidamente a inclinação vezes a taxa de aprendizado.\n",
    "\n",
    "Mas como escolhemos uma taxa de aprendizado?\n",
    "\n",
    "### Escolhendo uma Taxa de Aprendizado\n",
    "\n",
    "A **taxa de aprendizado** define o quão agressivamente você deseja que o algoritmo de descida do gradiente se mova em direção ao mínimo. É uma fração da inclinação subtraída do valor $ x $ repetidamente até que a função seja minimizada (se você estiver maximizando a função, some a inclinação vezes a taxa de aprendizado em vez de subtrair).\n",
    "\n",
    "Quanto maior a taxa de aprendizado, mais rápido o progresso ocorrerá, mas à custa da precisão. Se for muito grande, pode não convergir para o mínimo, pois seria como um gigante pisando no vale repetidamente. Tê-la muito pequena criará mais precisão, mas exigirá mais tempo e passos, como uma formiga descendo no vale. Você precisa encontrar um equilíbrio entre os dois. Experimente a taxa de aprendizagem acima (por exemplo, $ 0,3 $ versus $ 0,001 $) para ver como ela afeta o progresso da descida do gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a49cf8-ab69-41d1-9737-f47c2a412bff",
   "metadata": {},
   "source": [
    "## Descida de gradiente multivariável"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773611c-ff4c-4994-89db-f4ef7cb09a7a",
   "metadata": {},
   "source": [
    "Vamos analisar esta função multivariável e plotá-la.\n",
    "\n",
    "$\n",
    "f(x) = 5 x^{2} + 4 y^{2} + 1\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37132f21-48f4-42df-bcaf-411b533e4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "\n",
    "x, y = symbols('x y')\n",
    "\n",
    "f = 5*x**2 + 4*y**2 + 1 \n",
    "\n",
    "plot3d(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642aa64-b393-4775-8535-d6f0d6bb81d2",
   "metadata": {},
   "source": [
    "Novamente, podemos resolver algebricamente o mínimo, mas vamos praticar o uso da descida do gradiente com ela. Como aprendemos na última seção, podemos usar derivadas parciais para encontrar a derivada em relação a cada variável de entrada.\n",
    "\n",
    "$ \n",
    "\\Large \\frac{\\delta}{\\delta x} = 10x\n",
    "$   \n",
    "\n",
    "$ \n",
    "\\Large \\frac{\\delta}{\\delta y} = 8y\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8097a61-a68b-45d1-bf5d-cfd11024de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "\n",
    "x, y = symbols('x y')\n",
    "\n",
    "f = 5*x**2 + 4*y**2 + 1 \n",
    "dx = diff(f, x) \n",
    "dy = diff(f, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bda0ad-3c7a-4f87-8cb0-c4312d8f4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86aa80-e340-4eb3-a816-41fda755edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e29a58-6fd8-4378-aa64-b9c8415379b3",
   "metadata": {},
   "source": [
    "Vamos experimentar a descida do gradiente de maneira semelhante à que fizemos anteriormente. Primeiro, declaramos nossas derivadas parciais e uma taxa de aprendizado $ L = 0.05 $. Também começaremos $ x $ e $ y $ em um local aleatório no gráfico acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38277c9f-cc7e-458c-962a-5f244cfe9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "import random \n",
    "\n",
    "# declarar função e derivada\n",
    "x,y = symbols('x y')\n",
    "f = 5*x**2 + 4*y**2 + 1 \n",
    "dx = diff(f, x) \n",
    "dy = diff(f, y)\n",
    "\n",
    "# declarar taxa de aprendizagem\n",
    "L = .05\n",
    "\n",
    "# iniciar x em local aleatório\n",
    "x_i = random.uniform(-10,10)\n",
    "y_i = random.uniform(-10,10)\n",
    "\n",
    "# plot \n",
    "dx_i = dx.subs(x, x_i)\n",
    "dy_i = dy.subs(y, y_i) \n",
    "b = -(dx_i * x_i + dy_i * y_i - f.subs([(x, x_i), (y, y_i)]))\n",
    "\n",
    "plot3d(f, dx_i*x + dy_i * y + b, xlim=(-10,10),ylim=(-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45891e-e462-4b35-93b7-5aaf33ac51b6",
   "metadata": {},
   "source": [
    "Execute este bloco de código abaixo repetidamente e você verá o plano linear, capturando a inclinação de $ x $ e $ y $, se deslocando em direção ao mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b4b37-bd73-43e4-bbc3-b676feadb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_i = dx.subs(x, x_i)\n",
    "dy_i = dy.subs(y, y_i) \n",
    "\n",
    "x_i -= dx_i * L \n",
    "y_i -= dy_i * L \n",
    "\n",
    "b = -(dx_i * x_i + dy_i * y_i - f.subs([(x, x_i), (y, y_i)]))\n",
    "\n",
    "print(f\"x = {x_i}, y = {y_i}\")\n",
    "plot3d(f, dx_i*x + dy_i * y + b, xlim=(-10,10),ylim=(-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526f7d1-aa54-4473-93bc-52c809ed4bbe",
   "metadata": {},
   "source": [
    "Podemos reempacotar tudo isso em um único script Python para executar essa descida de gradiente. Você verá que $ x $ e $ y $ convergem muito próximos para $ (0.0) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2e906-ac3e-4bf7-9a7b-2886167f2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "import random \n",
    "\n",
    "# declarar função e derivada\n",
    "x,y = symbols('x y')\n",
    "f = 5*x**2 + 4*y**2 + 1 \n",
    "dx = diff(f, x) \n",
    "dy = diff(f, y)\n",
    "\n",
    "# declarar taxa de aprendizagem\n",
    "L = .05\n",
    "\n",
    "# iniciar x em local aleatório\n",
    "x_i = random.uniform(-10,10)\n",
    "y_i = random.uniform(-10,10)\n",
    "\n",
    "for i in range(1000):\n",
    "    dx_i = dx.subs(x, x_i)\n",
    "    dy_i = dy.subs(y, y_i) \n",
    "\n",
    "    x_i -= dx_i * L \n",
    "    y_i -= dy_i * L \n",
    "\n",
    "    b = -(dx_i * x_i + dy_i * y_i - f.subs([(x, x_i), (y, y_i)]))\n",
    "\n",
    "print(f\"x = {x_i}, y = {y_i}\")\n",
    "plot3d(f, dx_i*x + dy_i * y + b, xlim=(-10,10),ylim=(-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee87c41-7943-4596-90e4-d1b5a5d4f17e",
   "metadata": {},
   "source": [
    "## Gradiente descendente para regressão linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b1663-ea97-47c9-a213-576fa507df5e",
   "metadata": {},
   "source": [
    "Vamos aplicar a descida do gradiente a algo um pouco mais próximo da prática do mundo real. Embora a regressão linear tenha técnicas de atalho, como decomposição matricial, é uma boa maneira de entender a descida do gradiente para modelos de aprendizado de máquina baseados em dados. Afinal, redes neurais são compostas por funções lineares dentro de funções não lineares, cujas inclinações e interceptos (ou pesos e vieses) são otimizados com a descida do gradiente. Vamos praticar com uma única função linear.\n",
    "\n",
    "Uma regressão linear ajusta uma reta (ou plano linear, se houver múltiplas variáveis ​​de entrada) através de alguns dados. A **função de perda** é o que estamos tentando minimizar usando a descida do gradiente, e normalmente será a *soma dos quadrados* ou a *média dos quadrados*. Os **resíduos quadrados** são as diferenças quadradas entre o valor de $ y $ de cada ponto de dados e o valor $ y $ previsto da reta, que, quando somados ou calculados como média, compõem a função de perda.\n",
    "\n",
    "Aqui estão os resíduos quadrados visualizados abaixo para uma determinada reta e 15 pontos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41e5c2-729f-474d-8cf6-ef449ef64dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Extrair variáveis ​​de entrada (todas as linhas, todas as colunas, exceto a última coluna)\n",
    "X = np.array([9.8, 8.3, 5.3, 1.3, 3, 0.4, 5.4, 7.3, 3.7, 6.8, 5.6, 2,7.6, 7.9, 1.5])\n",
    "\n",
    "Y = np.array([8.383017, 7.35061323, 5.31904498, 0.99811892, 2.64478489, 1.12535641,\n",
    " 5.62574367, 6.82704871, 5.66768037, 6.98267837, 7.23655439, 3.36467504,\n",
    " 9.82253924, 8.52430761, 1.39760223])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "# declarar coeficientes de linha\n",
    "m, b = 0.9153874397162779, 0.7861238923689651\n",
    "\n",
    "# plotar quadrados\n",
    "for x,y in zip(X,Y): \n",
    "    residuo = m*x+b - y\n",
    "    ax.add_patch(Rectangle((x, y), residuo, residuo, alpha=.5, color='orange'))\n",
    "\n",
    "plt.plot(X, m*X+b)\n",
    "plt.plot(X, Y, 'o') # gráfico de dispersão\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a12e1-52cf-4c1b-b739-a82a0513c606",
   "metadata": {},
   "source": [
    "Vamos usar a soma dos quadrados (a soma de todas as áreas quadradas acima) como nossa função de perda.\n",
    "\n",
    "$\n",
    "\\Large \\text{SSE} = \\sum_{i=0}^{n} (m x_i + b - y_i)^{2}\n",
    "$ \n",
    "\n",
    "Para ver como é o cenário de perdas, vamos usar o SymPy. Como você pode imaginar, precisamos encontrar os valores de $ m $ e $ b $ que nos levarão ao ponto mais baixo deste gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab56da9-df28-481c-a7a2-4308c11ad92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "from sympy.plotting import plot3d\n",
    "import pandas as pd\n",
    "\n",
    "m, b, i, n = symbols('m b i n')\n",
    "x, y = symbols('x y', cls=Function)\n",
    "\n",
    "soma_dos_quadrados = Sum((m*x(i) + b - y(i)) ** 2, (i, 0, n)) \\\n",
    "    .subs(n, len(X) - 1).doit() \\\n",
    "    .replace(x, lambda i: X[i]) \\\n",
    "    .replace(y, lambda i: Y[i])\n",
    "\n",
    "plot3d(soma_dos_quadrados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd898cb-71a3-46a1-b848-cf703ff86681",
   "metadata": {},
   "source": [
    "Encontraremos a derivada da função de perda em relação a $ m $ e em relação a $ b $ usando o SymPy. Observe como podemos suportar múltiplos valores $ x $ e $ y $ especificando `cls=Function` para `symbols()`. Em seguida, usaremos o operador `Sum` para realizar uma soma que totaliza as diferenças quadradas entre os valores $ y $ reais e os valores $ y $ previstos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352b7bb-7dfa-4f40-bf50-a067f5fcb7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "\n",
    "m, b, i, n = symbols('m b i n')\n",
    "x, y = symbols('x y', cls=Function)\n",
    "\n",
    "soma_dos_quadrados = Sum((m*x(i) + b - y(i)) ** 2, (i, 0, n))\n",
    "\n",
    "d_m = diff(soma_dos_quadrados, m)\n",
    "d_b = diff(soma_dos_quadrados, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c716aa-d94c-4a5f-a7d6-35fe86ba10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d762d5-fc19-4e08-b62a-e07417b96d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc77262-5859-4bcd-addc-b857ecee1b5b",
   "metadata": {},
   "source": [
    "Podemos implementar essas duas derivadas manualmente no NumPy, como mostrado abaixo, e usá-lo para executar a descida do gradiente. Observe como isso se assemelha ao nosso exemplo anterior de descida do gradiente multivariável. Estamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19d2c3-cb9e-46d6-8f54-e06a3f24f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sympy import *\n",
    "\n",
    "X = np.array([9.8, 8.3, 5.3, 1.3, 3, 0.4, 5.4, 7.3, 3.7, 6.8, 5.6, 2,7.6, 7.9, 1.5])\n",
    "\n",
    "Y = np.array([8.383017, 7.35061323, 5.31904498, 0.99811892, 2.64478489, 1.12535641,\n",
    " 5.62574367, 6.82704871, 5.66768037, 6.98267837, 7.23655439, 3.36467504,\n",
    " 9.82253924, 8.52430761, 1.39760223])\n",
    "\n",
    "# Construindo o modelo\n",
    "m = 1.0\n",
    "b = 1.0\n",
    "\n",
    "# A taxa de aprendizagem\n",
    "L = .001\n",
    "\n",
    "# O número de iterações\n",
    "iterations = 100_000\n",
    "\n",
    "n = float(len(X))  # Número de elementos em X\n",
    "\n",
    "# Executar Descida de Gradiente\n",
    "for i in range(iterations):\n",
    "\n",
    "    # inclinação em relação a m\n",
    "    D_m = (2 * X * ((m * X + b) - Y)).sum()\n",
    "\n",
    "    # inclinação em relação a b\n",
    "    D_b = (2 * ((m * X + b) - Y)).sum()\n",
    "\n",
    "    # atualiza m e b\n",
    "    m -= L * D_m\n",
    "    b -= L * D_b\n",
    "print(m, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7be56-ba86-42fd-985e-447dc0ad1672",
   "metadata": {},
   "source": [
    "Se você quiser continuar usando o SymPy, basta substituir os pontos de dados nas funções derivadas. Pela natureza da implementação de soma no SymPy, você precisará chamar `doit()` e depois `lambdify()` para compilar as funções derivadas com eficiência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae64037-7be5-43ac-b6c5-1f9a541f0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sympy import *\n",
    "\n",
    "X = np.array([9.8, 8.3, 5.3, 1.3, 3, 0.4, 5.4, 7.3, 3.7, 6.8, 5.6, 2,7.6, 7.9, 1.5])\n",
    "\n",
    "Y = np.array([8.383017, 7.35061323, 5.31904498, 0.99811892, 2.64478489, 1.12535641,\n",
    " 5.62574367, 6.82704871, 5.66768037, 6.98267837, 7.23655439, 3.36467504,\n",
    " 9.82253924, 8.52430761, 1.39760223])\n",
    "\n",
    "\n",
    "m, b, i, n = symbols('m b i n')\n",
    "x, y = symbols('x y', cls=Function)\n",
    "\n",
    "sum_of_squares = Sum((m*x(i) + b - y(i)) ** 2, (i, 0, n))\n",
    "\n",
    "d_m = diff(sum_of_squares, m) \\\n",
    "    .subs(n, len(X) - 1).doit() \\\n",
    "    .replace(x, lambda i: X[i]) \\\n",
    "    .replace(y, lambda i: Y[i])\n",
    "\n",
    "d_b = diff(sum_of_squares, b) \\\n",
    "    .subs(n, len(X) - 1).doit() \\\n",
    "    .replace(x, lambda i: X[i]) \\\n",
    "    .replace(y, lambda i: Y[i])\n",
    "\n",
    "# compilar usando lambdify para computação mais rápida\n",
    "d_m = lambdify([m, b], d_m)\n",
    "d_b = lambdify([m, b], d_b)\n",
    "\n",
    "# Construindo o modelo\n",
    "m = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# A taxa de aprendizagem\n",
    "L = .001\n",
    "\n",
    "# O número de iterações\n",
    "iterations = 100_000\n",
    "\n",
    "# Executar Descida de Gradiente\n",
    "for i in range(iterations):\n",
    "\n",
    "    # atualiza m e b\n",
    "    m -= d_m(m,b) * L\n",
    "    b -= d_b(m,b) * L\n",
    "\n",
    "print(\"y = {0}x + {1}\".format(m, b))\n",
    "\n",
    "print(m, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547bd43-4c8b-41da-bf60-437a9738b9e2",
   "metadata": {},
   "source": [
    "Agora vamos dar uma olhada no resultado e plotá-lo. Parece muito bom! Essa reta parece se encaixar perfeitamente nos pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615004b7-34e8-49a8-bd4f-bab1e4a1f16d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# plotar quadrados\n",
    "for x,y in zip(X,Y): \n",
    "    residual = m*x+b - y\n",
    "    ax.add_patch(Rectangle((x, y), residual, residual, alpha=.5, color='orange'))\n",
    "\n",
    "plt.plot(X, m*X+b)\n",
    "plt.plot(X, Y, 'o') # gráfico de dispersão\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6583b1be-7303-4f22-9461-70f46097c89e",
   "metadata": {},
   "source": [
    "## Descida do gradiente estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f0368-1d65-4d8d-9f33-97abdcb0ebe8",
   "metadata": {},
   "source": [
    "Seria negligente não mencionar ao menos a **descida do gradiente estocástico**, uma variante da descida do gradiente que amostra aleatoriamente apenas um ou mais pontos de dados de treinamento em cada iteração. Isso ocorre porque percorrer todo o conjunto de dados pode ser computacionalmente custoso para conjuntos de dados maiores e modelos complexos, como o aprendizado profundo. Abaixo, amostramos aleatoriamente apenas um ponto de dados em cada iteração. Você notará que a reta não se ajusta tão agressivamente, e alguma aleatoriedade produzirá valores diferentes de $ m $ e $ b $ a cada vez. Isso provavelmente não é problema, pois outro objetivo é evitar o sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e585128-b96a-4c06-8620-373d33c7050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random \n",
    "from sympy import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([9.8, 8.3, 5.3, 1.3, 3, 0.4, 5.4, 7.3, 3.7, 6.8, 5.6, 2,7.6, 7.9, 1.5])\n",
    "\n",
    "Y = np.array([8.383017, 7.35061323, 5.31904498, 0.99811892, 2.64478489, 1.12535641,\n",
    " 5.62574367, 6.82704871, 5.66768037, 6.98267837, 7.23655439, 3.36467504,\n",
    " 9.82253924, 8.52430761, 1.39760223])\n",
    "\n",
    "# Construindo o modelo\n",
    "m = 1.0\n",
    "b = 1.0\n",
    "\n",
    "# A taxa de aprendizagem\n",
    "L = .001\n",
    "\n",
    "# O número de iterações\n",
    "iterations = 100_000\n",
    "\n",
    "n = float(len(X))  # Número de elementos em X\n",
    "\n",
    "# Executar Descida de Gradiente\n",
    "for i in range(iterations):\n",
    "    j = random.randint(0,len(X)-1)\n",
    "    _x, _y = X[j], Y[j]\n",
    "    \n",
    "    # inclinação em relação a m\n",
    "    D_m = 2 * _x * ((m * _x + b) - _y)\n",
    "\n",
    "    # inclinação em relação a b\n",
    "    D_b = 2 * ((m * _x + b) - _y)\n",
    "\n",
    "    # atualiza m e b\n",
    "    m -= L * D_m\n",
    "    b -= L * D_b\n",
    "    \n",
    "print(m, b)\n",
    "\n",
    "# plotar o resultado\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.plot(X, m*X+b)\n",
    "plt.plot(X, Y, 'o') # gráfico de dispersão\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db51bb4-5c79-4535-9e34-3707d44c82c7",
   "metadata": {},
   "source": [
    "## EXERCÍCIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b84341-069c-43d6-b092-77863a8616df",
   "metadata": {},
   "source": [
    "Abaixo, temos uma função que aceita as variáveis ​​de entrada $ x $ e $ y $.\n",
    "\n",
    "$ \\Large f(x,y) = 3 \\left(x + 2\\right)^{2} + 0.5 \\left(y - 1\\right)^{2} $\n",
    "\n",
    "Encontre os valores de $ x $ e $ y $ que produzem o menor valor nessa função usando o gradiente descendente e, em seguida, plote-o. Preencha o código abaixo substituindo os pontos de interrogação \"?\" e experimentando com a taxa de aprendizado e as iterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dff00-c695-46ee-a404-b397c4280a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "import random \n",
    "\n",
    "# declara função e derivada\n",
    "x,y = symbols('x y')\n",
    "f = 3*(x+2)**2 + .5*(y-1)**2\n",
    "dx = diff(f, x) \n",
    "dy = diff(f, y)\n",
    "\n",
    "# declara taxa de aprendizagem\n",
    "L = ?\n",
    "\n",
    "# inicia x em local aleatório\n",
    "x_i = random.uniform(-10,10)\n",
    "y_i = random.uniform(-10,10)\n",
    "\n",
    "for i in range(?):\n",
    "    dx_i = dx.subs(x, x_i)\n",
    "    dy_i = dy.subs(y, y_i) \n",
    "\n",
    "    x_i -= dx_i * L \n",
    "    y_i -= dy_i * L \n",
    "\n",
    "    b = -(dx_i * x_i + dy_i * y_i - f.subs([(x, x_i), (y, y_i)]))\n",
    "\n",
    "# imprime e plota o resultado\n",
    "print(f\"x = {x_i}, y = {y_i}\")\n",
    "plot3d(f, dx_i*x + dy_i * y + b, xlim=(-10,10),ylim=(-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6e0d0-49f6-48e3-8096-36ca92cfad64",
   "metadata": {},
   "source": [
    "### RESPOSTA A BAIXO\n",
    "\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af617887-fc8c-4b92-818b-78bc61d27fee",
   "metadata": {},
   "source": [
    "Você deve convergir em $ x = -2 $ e $ y = 1 $. Uma taxa de aprendizado de $ 0,05 $ e $ 1.000 iterações devem ser suficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c1978-6859-4c53-8262-6f7edc584033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "import random \n",
    "\n",
    "# declara função e derivada\n",
    "x,y = symbols('x y')\n",
    "f = 3*(x+2)**2 + .5*(y-1)**2\n",
    "dx = diff(f, x) \n",
    "dy = diff(f, y)\n",
    "\n",
    "# declara taxa de aprendizagem \n",
    "L = .05\n",
    "\n",
    "# inicia x em local aleatório\n",
    "x_i = random.uniform(-10,10)\n",
    "y_i = random.uniform(-10,10)\n",
    "\n",
    "for i in range(1000):\n",
    "    dx_i = dx.subs(x, x_i)\n",
    "    dy_i = dy.subs(y, y_i) \n",
    "\n",
    "    x_i -= dx_i * L \n",
    "    y_i -= dy_i * L \n",
    "\n",
    "    b = -(dx_i * x_i + dy_i * y_i - f.subs([(x, x_i), (y, y_i)]))\n",
    "\n",
    "# imprime e plota o resultado\n",
    "print(f\"x = {x_i}, y = {y_i}\")\n",
    "plot3d(f, dx_i*x + dy_i * y + b, xlim=(-10,10),ylim=(-10,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
