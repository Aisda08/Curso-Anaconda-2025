{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918b7b0a-bc9b-43ec-b46c-02e507a682f6",
   "metadata": {},
   "source": [
    "# Estimativa de Máxima Verossimilhança"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3650b6-2fe4-4726-b115-2dda19605c09",
   "metadata": {},
   "source": [
    "Digamos que eu tenha alguns dados e queira modificar uma função para que ela se ajuste o máximo possível aos dados. Esqueça isso. Quero *maximizar a probabilidade* de a função gerar os dados que estou observando. Isso é exatamente o que significa a **estimativa de máxima verossimilhança**, onde encontramos a probabilidade conjunta de uma determinada função gerar alguns dados observados. Essa técnica é usada para ajustar distribuições de probabilidade a um determinado conjunto de dados, bem como para treinar algoritmos de aprendizado de máquina, como regressão linear e regressão logística.\n",
    "\n",
    "Aprenderemos sobre a estimativa de máxima verossimilhança aplicando-a primeiro a uma distribuição normal. Em seguida, estenderemos essa estrutura para ajustar uma regressão linear. É claro que uma regressão linear pode ser ajustada usando mínimos quadrados, mas, como aprenderemos, a estimativa de máxima verossimilhança chegará a uma solução quase idêntica. Dado que a regressão linear é a base e o bloco de construção do aprendizado de máquina, isso conectará ideias sobre como a probabilidade desempenha um papel no aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e8d03-0fbf-4810-9a79-06aef5583123",
   "metadata": {},
   "source": [
    "## Ajustando uma distribuição normal## Fitting a Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0a441-7ecc-4325-b63a-1eb8b9696651",
   "metadata": {},
   "source": [
    "Vamos observar algumas medições de um processo de usinagem em uma oficina e plotá-las em uma reta numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aea1f3-8cb5-4e31-a801-02150aefa1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([54.4,55.8,55.9,58.5,59.1,61.1,61.3,61.7,62.8,\n",
    "              63.2,63.5,63.6,63.6,63.7,64.0,64.2,65.0,65.0,\n",
    "              65.7,66.0,66.2,66.5,67.7,67.7,68.2,69.4,69.5,\n",
    "              70.2,70.5,71.8,72.8,72.8,73.8,76.2,77.4])\n",
    "\n",
    "plt.plot(X, [0 for _ in X], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a06b10-264f-42d0-83cc-62353bc889fa",
   "metadata": {},
   "source": [
    "Suspeitamos que esses dados seguem uma distribuição normal e queremos ajustá-la a uma. Embora pudéssemos simplesmente pegar a média e o desvio padrão dos dados e usá-los em nossos parâmetros para uma distribuição normal, vamos adotar uma abordagem mais probabilística com estimativa de máxima verossimilhança. Outras distribuições, como a distribuição exponencial, precisariam adotar essa abordagem, assim como o ajuste de uma regressão linear ou logística.\n",
    "\n",
    "Temos muito o que analisar. Vamos primeiro discutir a ideia de verossimilhança conjunta para esse propósito. Observe os pontos de dados e uma distribuição normal fornecida abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2364f-2ea4-4e67-86ea-7fdde8088b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([54.4,55.8,55.9,58.5,59.1,61.1,61.3,61.7,62.8,\n",
    "              63.2,63.5,63.6,63.6,63.7,64.0,64.2,65.0,65.0,\n",
    "              65.7,66.0,66.2,66.5,67.7,67.7,68.2,69.4,69.5,\n",
    "              70.2,70.5,71.8,72.8,72.8,73.8,76.2,77.4])\n",
    "\n",
    "mu, sigma = 65.696, 5.469\n",
    "\n",
    "# plota a distribuição e os pontos\n",
    "x_range = np.arange(start=-3 * sigma + mu, stop=3 * sigma + mu, step=.01)\n",
    "plt.plot(X, [0 for _ in X], 'o')\n",
    "plt.plot(x_range, norm.pdf(x_range, mu, sigma))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd195b4-65d6-48d5-9931-3f21f2feb4d8",
   "metadata": {},
   "source": [
    "Imagine cada um desses pontos projetando-se para cima na curva, e os valores y resultantes são as probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78f631-a81f-4b44-a6e4-abc8d2feb188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_range, norm.pdf(x_range, mu, sigma), color='orange')\n",
    "\n",
    "# plota linhas\n",
    "for x in X:\n",
    "    plt.plot(np.array([x,x]),\n",
    "              np.array([0, norm.pdf(x, mu, sigma)]),\n",
    "             'bo', linestyle=\"--\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac18ad-b24f-43fd-867e-32e841e2c71d",
   "metadata": {},
   "source": [
    "Os valores de y resultantes que se assemelham às probabilidades são o que queremos multiplicar, chamado de **probabilidade conjunta**. Podemos calculá-la usando a função `prod()` em um array NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32e552-1a30-4f4e-9b90-0cdcf354e7f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "X = np.array([54.4, 55.8, 55.9, 58.5, 59.1, 61.1, 61.3, 61.7, 62.8,\n",
    "              63.2, 63.5, 63.6, 63.6, 63.7, 64.0, 64.2, 65.0, 65.0,\n",
    "              65.7, 66.0, 66.2, 66.5, 67.7, 67.7, 68.2, 69.4, 69.5,\n",
    "              70.2, 70.5, 71.8, 72.8, 72.8, 73.8, 76.2, 77.4])\n",
    "\n",
    "media, desvio = 65.696, 5.469\n",
    "\n",
    "verossimilhancas = norm.pdf(X, media, desvio)\n",
    "verossimilhanca_conjunta = norm.pdf(X, media, desvio).prod()\n",
    "\n",
    "print(f\"Verossimilhanças: {verossimilhancas}\")\n",
    "print(f\"\\nVerossimilhança Conjunta: {verossimilhanca_conjunta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee689f-4dca-47f2-a903-74791f7c5825",
   "metadata": {},
   "source": [
    "Essa probabilidade conjunta provavelmente ficará muito pequena, como mostrado acima, pois estamos multiplicando muitas probabilidades. Nos bastidores, usamos a soma de logaritmos de verossimilhança (em vez da multiplicação) para evitar subfluxos de ponto flutuante, mas deixaremos o NumPy fazer esse trabalho. A questão é que queremos ajustar mu e sigma para maximizar essa probabilidade conjunta total (portanto, a estimativa de máxima verossimilhança)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1777f5-8517-4a3a-8fba-182a87ae814a",
   "metadata": {},
   "source": [
    "Para aplicar a estimativa de máxima verossimilhança, precisamos primeiro aprender um algoritmo de otimização. Normalmente, usaríamos uma técnica como gradiente descendente ou gradiente descendente estocástico. No entanto, para evitar a necessidade de um curso intensivo de Cálculo (mais um curso da Anaconda sobre isso depois!), usaremos um algoritmo mais simples chamado hill climbing. O hill climbing nos permite usar uma busca aleatória, porém metódica, que faz ajustes aleatórios nos valores, mas aceita apenas valores que melhoram em direção a um objetivo. Nesse caso, o objetivo é a máxima verossimilhança conjunta total.\n",
    "\n",
    "Lembre-se de que uma distribuição normal aceita os parâmetros *mu* $ \\mu $ e *sigma* $ \\sigma $. Faremos ajustes aleatórios repetidamente nesses dois parâmetros e aceitaremos apenas ajustes que melhorem a máxima verossimilhança total. Mas quais serão os ajustes aleatórios? Não queremos necessariamente usar força bruta com valores uniformemente aleatórios, mas podemos amostrar a partir de uma distribuição normal padrão (com média de 0 e desvio padrão de 1) para obtermos principalmente pequenos ajustes próximos de 0, mas ocasionalmente grandes ajustes na cauda. Observe a distribuição normal padrão abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf225b79-3f58-4c59-866c-65389b2ec30c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "x_range = np.arange(start=-3, stop=3, step=.01)\n",
    "plt.plot(x_range, norm.pdf(x_range, loc=0, scale=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143720ce-eb34-4dcc-8194-67e6aef5eb4a",
   "metadata": {},
   "source": [
    "Pode parecer bastante meta (e conceitualmente tortuoso) usarmos uma distribuição normal para ajustar aleatoriamente outra distribuição normal. Mas funciona! Lembre-se de que uma é a distribuição normal que estamos ajustando aos nossos dados, e outra distribuição normal gera ajustes aleatórios para $ \\mu $ e $ \\sigma $.\n",
    "\n",
    "Vamos juntar esses conceitos e usar a técnica de hill climbing para executar a estimativa de máxima verossimilhança. Começaremos $ \\mu $ em um dos pontos de dados e o desvio padrão em 1. Eles precisam começar aproximadamente em algum lugar próximo aos pontos e, então, a técnica de hill climbing os ajustará de acordo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed96d03-f7a0-4239-ae7d-f0529fe73d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# declarar os pontos de dados X\n",
    "X = np.array([54.4, 55.8, 55.9, 58.5, 59.1, 61.1, 61.3, 61.7, 62.8,\n",
    "              63.2, 63.5, 63.6, 63.6, 63.7, 64.0, 64.2, 65.0, 65.0,\n",
    "              65.7, 66.0, 66.2, 66.5, 67.7, 67.7, 68.2, 69.4, 69.5,\n",
    "              70.2, 70.5, 71.8, 72.8, 72.8, 73.8, 76.2, 77.4])\n",
    "\n",
    "# iniciar a média no primeiro ponto de dado e o desvio padrão em 1\n",
    "# não importa onde eles comecem, apenas que estejam\n",
    "# em algum lugar dentro ou próximo ao conjunto de dados\n",
    "media, desvio = X[0], 1\n",
    "\n",
    "# gera um valor aleatório de uma distribuição normal padrão\n",
    "# onde a média é 0 e o desvio padrão é 1\n",
    "def ajuste_aleatorio(): return np.random.normal(loc=0, scale=1, size=1)[0]\n",
    "\n",
    "# iniciar a melhor verossimilhança conjunta em 0\n",
    "melhor_verossimilhanca_conjunta = 0\n",
    "\n",
    "# fazer 10.000 ajustes aleatórios em média e desvio\n",
    "for i in range(10_000):\n",
    "\n",
    "    # ajustar aleatoriamente média e desvio\n",
    "    ajuste_media, ajuste_desvio = ajuste_aleatorio(), ajuste_aleatorio()\n",
    "\n",
    "    media += ajuste_media\n",
    "    desvio += ajuste_desvio\n",
    "\n",
    "    # calcular a nova verossimilhança conjunta de todos os pontos de dados\n",
    "    nova_verossimilhanca_conjunta = norm.pdf(X, media, desvio).prod()\n",
    "\n",
    "    # se a verossimilhança conjunta melhorar, manter as mudanças\n",
    "    if nova_verossimilhanca_conjunta > melhor_verossimilhanca_conjunta:\n",
    "        melhor_verossimilhanca_conjunta = nova_verossimilhanca_conjunta\n",
    "        print(f\"média, desvio -> {media}, {desvio}\")\n",
    "    else:\n",
    "        # caso contrário, desfazer as mudanças\n",
    "        media -= ajuste_media\n",
    "        desvio -= ajuste_desvio\n",
    "\n",
    "# plotar o resultado\n",
    "intervalo_x = np.arange(start=-3 * desvio + media, stop=3 * desvio + media, step=.01)\n",
    "\n",
    "plt.plot(X, [0 for _ in X], 'o')\n",
    "plt.plot(intervalo_x, norm.pdf(intervalo_x, media, desvio))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d8c4a-9e03-4fb2-a60f-60778fbd7067",
   "metadata": {},
   "source": [
    "Com base nas saídas de impressão e no gráfico acima, você pode ver que a curva se moveu efetivamente para se ajustar aos pontos. Observe também que, se tomarmos a média e o desvio padrão dos dados, eles correspondem ao que a estimativa de máxima verossimilhança encontrou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370355c9-af9a-4921-81fd-c86e5a1e62eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.mean(), X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753e200-fa75-4596-a9f7-e8d9907aa109",
   "metadata": {},
   "source": [
    "Mas este não foi apenas um exercício acadêmico desnecessário. Você pode ajustar qualquer distribuição de probabilidade com base em dados usando esta técnica.\n",
    "\n",
    "Agora, vamos estender essa ideia ao ajuste de uma regressão linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f744c-e7ef-435d-bfe4-90c81dd88a3c",
   "metadata": {},
   "source": [
    "## Usando MLE para ajustar uma regressão linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee18a5-793d-41b5-83f2-c04176a6bbaf",
   "metadata": {},
   "source": [
    "A seguir, vamos usar a estimativa de máxima verossimilhança para ajustar uma regressão linear, que consiste em ajustar uma reta passando por alguns pontos. Observe os dados abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ade71-6a7a-40f8-9ddb-d0ab5926b6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([2.9,9.5,5.1,0.9,2.0,2.3,5.2,7.7,7.9,4.1,9.7,6.3,4.9,6.2,4.2,\n",
    "              3.1,3.9,8.7,1.2,9.6,0.8,3.0,5.6,7.3,3.7,3.5,2.6,5.0,1.7,9.1,1.9,2.4,0.3,5.7,9.0])\n",
    "\n",
    "Y = np.array([7.58,18.83,8.71,0.6,6.06,0.64,12.09,13.65,15.34,8.6,16.32,11.78,9.78,8.44,9.18,\n",
    "              3.04,7.65,10.96,1.47,17.52,2.21,4.76,13.03,12.29,10.2,7.88,3.01,8.92,2.23,15.08,\n",
    "              5.42,5.53,-0.5,11.66,14.7])\n",
    "\n",
    "\n",
    "plt.plot(X, Y, 'o') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a81faf-1b10-409c-9af5-f3543d708cb0",
   "metadata": {},
   "source": [
    "Queremos traçar uma reta através desses pontos com o objetivo de analisar/prever a relação entre duas variáveis ​​$ X $ e $ Y $. Lembre-se da fórmula para uma função linear.\n",
    "\n",
    "$\n",
    "y = mx + b\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18774257-b639-4175-ab39-c6abea882321",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.arange(start=X.min(), stop=X.max(), step=.01)\n",
    "plt.plot(X, Y, 'o') \n",
    "plt.plot(x_range, 1.71160239*x_range + 0.53778288) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590f557-ff58-487b-9287-8b25b734ade3",
   "metadata": {},
   "source": [
    "Imagine que uma distribuição normal segue essa linha, onde a média seguirá essa linha, mas o desvio padrão permanecerá constante. Podemos tratar os valores reais de y como a variável de entrada (o que normalmente é *x* na PDF) e os valores previstos de y como a média. Isso significa que precisamos calcular a inclinação *m*, o intercepto *b* e o desvio padrão $ \\sigma $. Como ajustar aleatoriamente três coeficientes de uma só vez é uma grande mudança, selecionaremos aleatoriamente apenas um deles para ajustar em cada iteração.\n",
    "\n",
    "Além dessas poucas mudanças, estamos realmente ajustando uma distribuição normal usando a estimativa de máxima verossimilhança, como antes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e5734-7cb8-40a0-9512-ff06fe1eb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# declarar os pontos de dados X e Y\n",
    "X = np.array([2.9, 9.5, 5.1, 0.9, 2.0, 2.3, 5.2, 7.7, 7.9, 4.1, 9.7, 6.3, 4.9, 6.2, 4.2,\n",
    "              3.1, 3.9, 8.7, 1.2, 9.6, 0.8, 3.0, 5.6, 7.3, 3.7, 3.5, 2.6, 5.0, 1.7, 9.1,\n",
    "              1.9, 2.4, 0.3, 5.7, 9.0])\n",
    "\n",
    "Y = np.array([7.58, 18.83, 8.71, 0.6, 6.06, 0.64, 12.09, 13.65, 15.34, 8.6, 16.32, 11.78, 9.78, 8.44, 9.18,\n",
    "              3.04, 7.65, 10.96, 1.47, 17.52, 2.21, 4.76, 13.03, 12.29, 10.2, 7.88, 3.01, 8.92, 2.23, 15.08,\n",
    "              5.42, 5.53, -0.5, 11.66, 14.7])\n",
    "\n",
    "m, b, desvio = 1, 1, 1\n",
    "\n",
    "# gera um valor aleatório de uma distribuição normal padrão\n",
    "# onde a média é 0 e o desvio padrão é 1\n",
    "def ajuste_aleatorio(): return np.random.normal(loc=0, scale=1, size=1)[0]\n",
    "\n",
    "# iniciar a melhor verossimilhança conjunta em 0\n",
    "melhor_verossimilhanca_conjunta = 0\n",
    "\n",
    "# fazer 30.000 ajustes aleatórios em m, b e desvio\n",
    "for i in range(30_000):\n",
    "\n",
    "    # ajustar aleatoriamente m, b ou desvio\n",
    "    ajuste = ajuste_aleatorio()\n",
    "    coef_aleatorio = np.random.randint(0, 3)\n",
    "    if coef_aleatorio == 0:\n",
    "        m += ajuste\n",
    "    elif coef_aleatorio == 1:\n",
    "        b += ajuste\n",
    "    elif coef_aleatorio == 2:\n",
    "        desvio += ajuste\n",
    "\n",
    "    # calcular a nova verossimilhança conjunta de todos os pontos de dados\n",
    "    nova_verossimilhanca_conjunta = np.array([norm.pdf(y, m * x + b, desvio) for x, y in zip(X, Y)]).prod()\n",
    "\n",
    "    # se a verossimilhança conjunta melhorar, manter as mudanças\n",
    "    if nova_verossimilhanca_conjunta > melhor_verossimilhanca_conjunta:\n",
    "        melhor_verossimilhanca_conjunta = nova_verossimilhanca_conjunta\n",
    "        print(f\"m, b, desvio -> {m}, {b}, {desvio}\")\n",
    "    else:\n",
    "        # caso contrário, desfazer as mudanças\n",
    "        if coef_aleatorio == 0:\n",
    "            m -= ajuste\n",
    "        elif coef_aleatorio == 1:\n",
    "            b -= ajuste\n",
    "        elif coef_aleatorio == 2:\n",
    "            desvio -= ajuste\n",
    "\n",
    "# plotar o resultado\n",
    "intervalo_x = np.arange(start=X.min(), stop=X.max(), step=.01)\n",
    "\n",
    "plt.plot(X, Y, 'o')\n",
    "plt.plot(intervalo_x, m * intervalo_x + b)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daac7f2-46c4-49a7-a1dd-f0ca06440bc1",
   "metadata": {},
   "source": [
    "Se compararmos com um método convencional de mínimos quadrados, obtemos um resultado quase idêntico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e013915-a4db-4737-835e-31e6702ff096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(X.reshape([-1, 1]), Y)\n",
    "print(f\"m: {lr.coef_[0]}\")                            \n",
    "print(f\"b: {lr.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11294ca6-d4fa-4447-a1cd-ff8680549b98",
   "metadata": {},
   "source": [
    "## Exercício"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1cf17-556e-4049-a217-dcf9e851e652",
   "metadata": {},
   "source": [
    "Um bar está com trânsito lento às quartas-feiras e o gerente está pensando em adicionar uma promoção de happy hour.\n",
    "\n",
    "![img](data:image/svg+xml;base64,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)\n",
    "\n",
    "Ele quer entender quantas chegadas de clientes ocorrem, em média, a cada hora. Ele pede ao barman que registre 12 chegadas de clientes e quanto tempo decorreu entre cada uma (em horas). Complete o código abaixo (substituindo os pontos de interrogação \"?\") para realizar a estimativa de máxima verossimilhança e encontrar o parâmetro `lambda` (o número médio de clientes por hora) na distribuição exponencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e6b25-a2e1-4e9e-bce7-e9eb039ee53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "\n",
    "# tempos observados entre cada cliente\n",
    "X = np.array([0.27922493, 0.44124056, 0.50967118, 0.44413533, 0.67243048, 0.01870771,\n",
    "              0.08661839, 0.29967495, 1.68386979, 0.30475119, 0.65567402, 0.0098742\n",
    "])\n",
    "\n",
    "# iniciar com o tempo médio entre cada cliente, começando em 1\n",
    "taxa_lambda = 1\n",
    "\n",
    "# iniciar a melhor verossimilhança\n",
    "melhor_verossimilhanca = 0\n",
    "\n",
    "# realizar hill climbing e ajustar a taxa lambda\n",
    "# para melhorar a verossimilhança conjunta\n",
    "for i in range(?):\n",
    "    ajuste_aleatorio = np.random.normal(loc=?, scale=?, size=1)[0]\n",
    "    taxa_lambda += ?\n",
    "\n",
    "    nova_verossimilhanca = expon.pdf(?, scale=1 / taxa_lambda).prod()\n",
    "    if ? < ?:\n",
    "        melhor_verossimilhanca = nova_verossimilhanca\n",
    "    else:\n",
    "        taxa_lambda -= ?\n",
    "\n",
    "print(taxa_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19a85c-747c-406c-b871-46c85306231e",
   "metadata": {},
   "source": [
    "### RESPOSTA A BAIXO\n",
    "\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c7b08-b8fd-4912-83ca-8c52bbad8d09",
   "metadata": {},
   "source": [
    "Você deve encontrar aproximadamente 2,21 clientes por hora, considerando esses intervalos. Use a técnica de hill climbing e a estimativa de máxima verossimilhança da mesma forma que fizemos para a distribuição normal para resolver o parâmetro lambda $ \\lambda $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8eddf-f475-4364-86d3-ca50b5b0141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "\n",
    "# tempos observados entre cada cliente\n",
    "X = np.array([0.27922493, 0.44124056, 0.50967118, 0.44413533, 0.67243048, 0.01870771,\n",
    "              0.08661839, 0.29967495, 1.68386979, 0.30475119, 0.65567402, 0.0098742\n",
    "])\n",
    "\n",
    "# começar com o tempo médio entre cada cliente, iniciando em 1\n",
    "taxa_lambda = 1\n",
    "\n",
    "# iniciar a melhor verossimilhança\n",
    "melhor_verossimilhanca = 0\n",
    "\n",
    "# realizar hill climbing e ajustar a taxa lambda\n",
    "# para melhorar a verossimilhança conjunta\n",
    "for i in range(10_000):\n",
    "    ajuste_aleatorio = np.random.normal(loc=0, scale=1, size=1)[0]\n",
    "    taxa_lambda += ajuste_aleatorio\n",
    "\n",
    "    nova_verossimilhanca = expon.pdf(X, scale=1 / taxa_lambda).prod()\n",
    "    if melhor_verossimilhanca < nova_verossimilhanca:\n",
    "        melhor_verossimilhanca = nova_verossimilhanca\n",
    "    else:\n",
    "        taxa_lambda -= ajuste_aleatorio\n",
    "\n",
    "print(taxa_lambda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
