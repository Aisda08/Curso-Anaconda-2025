{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1300efd7166c362a",
   "metadata": {},
   "source": [
    "# Analytic Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6912653075291944",
   "metadata": {},
   "source": [
    "**Analytic functions**, also known as **windowing functions**, are a powerful tool in SQL for a record to attach contexts from other records. This will make sense with several examples we demonstrate. While we will show simpler ways to achieve previous tasks we have done previously with subqueries, derived tables, and common table expressions, all of these other approaches we learned are still highly flexible and necessary to know. But as we will see, common analytic operations often can be done with these windowing functions rather than subquerying tools. \n",
    "\n",
    "Let's set up first with the `company_operations.db` database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c47be84fdfaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd \n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "conn = sqlite3.connect('company_operations.db')\n",
    "pd.read_sql(\"SELECT * FROM WEATHER_MONITOR LIMIT 10\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62083bada33a8b3",
   "metadata": {},
   "source": [
    "## PARTITION BY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54155b854e4d9a",
   "metadata": {},
   "source": [
    "Let's say along every `WEATHER_MONITOR` record, we wanted to also show the average `TEMPERATURE` for that record's `YEAR` and `MONTH`. Previously we would use a subquery, derived table, or common table expression to achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbae8fa18b43329",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "WITH temp_avgs AS (\n",
    "    SELECT strftime('%Y', REPORT_DATE) AS YEAR, \n",
    "    strftime('%m', REPORT_DATE) AS MONTH,\n",
    "    AVG(TEMPERATURE) AS AVG_TEMP \n",
    "    FROM WEATHER_MONITOR\n",
    "    GROUP BY 1, 2\n",
    ") \n",
    "\n",
    "SELECT ID, \n",
    "REPORT_CODE, \n",
    "REPORT_DATE, \n",
    "LOCATION_ID, \n",
    "TEMPERATURE, \n",
    "AVG_TEMP\n",
    "\n",
    "FROM WEATHER_MONITOR INNER JOIN temp_avgs\n",
    "\n",
    "ON strftime('%Y', REPORT_DATE) = temp_avgs.YEAR\n",
    "AND strftime('%m', REPORT_DATE) = temp_avgs.MONTH\n",
    "\"\"\"\n",
    "            \n",
    "pd.read_sql(sql, conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2958d8746d45462",
   "metadata": {},
   "source": [
    "While common table expressions and subqueries are highly useful and customizable, this specific task is so common there are special functions and operators for it. Instead of doing all this common table expression and join work, we can take the average temperature `AVG(TEMPERATURE)` but `PARTITION` it over all records sharing the same year and month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba5ba72b903bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT ID, \n",
    "REPORT_CODE, \n",
    "REPORT_DATE, \n",
    "LOCATION_ID, \n",
    "TEMPERATURE, \n",
    "AVG(TEMPERATURE) OVER (PARTITION BY strftime('%Y', REPORT_DATE), strftime('%m', REPORT_DATE)) AS AVG_TEMP_Y_M\n",
    "\n",
    "FROM WEATHER_MONITOR \n",
    "\n",
    "ORDER BY ID\n",
    "\"\"\"\n",
    "            \n",
    "pd.read_sql(sql, conn, index_col='ID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5447e44b23f75d",
   "metadata": {},
   "source": [
    "What is particularly powerful about windowing functions like `PARTITION BY` is we can mix and match different scopes and contexts, with familiar aggregate functions like `MIN`, `MAX`, `AVG`, `SUM`, and `COUNT`. Below we add a few more analytic fields getting the average, min, and max temperatures for each record's `LOCATION_ID`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cbf5e68e1b66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT ID, \n",
    "REPORT_CODE, \n",
    "REPORT_DATE, \n",
    "LOCATION_ID, \n",
    "TEMPERATURE, \n",
    "AVG(TEMPERATURE) OVER (PARTITION BY strftime('%Y', REPORT_DATE), strftime('%m', REPORT_DATE)) AS AVG_TEMP_Y_M,\n",
    "AVG(TEMPERATURE) OVER (PARTITION BY LOCATION_ID) AVG_TEMP_LOCATION, \n",
    "MIN(TEMPERATURE) OVER (PARTITION BY LOCATION_ID) MIN_TEMP_LOCATION,\n",
    "MAX(TEMPERATURE) OVER (PARTITION BY LOCATION_ID) MAX_TEMP_LOCATION\n",
    "\n",
    "FROM WEATHER_MONITOR \n",
    "\n",
    "ORDER BY ID\n",
    "\"\"\"\n",
    "            \n",
    "pd.read_sql(sql, conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded9ea8ef531908",
   "metadata": {},
   "source": [
    "We can also reuse windowing clauses and alias them using the `WINDOW` keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3d1778e8a507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT ID, \n",
    "REPORT_CODE, \n",
    "REPORT_DATE, \n",
    "LOCATION_ID, \n",
    "TEMPERATURE, \n",
    "AVG(TEMPERATURE) OVER ym AS AVG_TEMP_Y_M,\n",
    "AVG(TEMPERATURE) OVER loc AVG_TEMP_LOCATION, \n",
    "MIN(TEMPERATURE) OVER loc MIN_TEMP_LOCATION,\n",
    "MAX(TEMPERATURE) OVER loc MAX_TEMP_LOCATION\n",
    "\n",
    "FROM WEATHER_MONITOR \n",
    "\n",
    "WINDOW ym AS (PARTITION BY strftime('%Y', REPORT_DATE), strftime('%m', REPORT_DATE)),\n",
    "loc AS (PARTITION BY LOCATION_ID)\n",
    "\n",
    "ORDER BY ID\n",
    "\"\"\"\n",
    "            \n",
    "pd.read_sql(sql, conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e94d8a2686105",
   "metadata": {},
   "source": [
    "Keep in mind that windowing functions like `PARTITION BY` will only scan records that pass the `WHERE` condition. This means if you need to reach out to records that exist outside the `WHERE` condition, you will need to go back to using subqueries and common table expressions. Notice how putting a `WHERE` condition on the query above for a single `REPORT_CODE` choked all the other data from the windowing functions, making all the statistical values `50` across the board since there is now only one datapoint.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e751eb9615eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT ID, \n",
    "REPORT_CODE, \n",
    "REPORT_DATE, \n",
    "LOCATION_ID, \n",
    "TEMPERATURE, \n",
    "AVG(TEMPERATURE) OVER ym AS AVG_TEMP_Y_M,\n",
    "AVG(TEMPERATURE) OVER loc AVG_TEMP_LOCATION, \n",
    "MIN(TEMPERATURE) OVER loc MIN_TEMP_LOCATION,\n",
    "MAX(TEMPERATURE) OVER loc MAX_TEMP_LOCATION\n",
    "\n",
    "FROM WEATHER_MONITOR \n",
    "WHERE REPORT_CODE = 'UVYMMWW' \n",
    "\n",
    "WINDOW ym AS (PARTITION BY strftime('%Y', REPORT_DATE), strftime('%m', REPORT_DATE)),\n",
    "loc AS (PARTITION BY LOCATION_ID)\n",
    "\"\"\"\n",
    "            \n",
    "pd.read_sql(sql, conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ce66c3d10935a",
   "metadata": {},
   "source": [
    "## ORDER BY "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1ab972fec911f",
   "metadata": {},
   "source": [
    "Here is another useful application of windowing functions. Recall we can use self joins with inequality join conditions to, for example, get a rolling total of orders. Assuming the `CUSTOMER_ORDER_ID` reflects when orders chronologically came in, I can query for records previous to each one and sum them as a `ROLLING_QTY`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22effda89c57b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT c1.CUSTOMER_ORDER_ID, \n",
    "c1.ORDER_DATE,\n",
    "c1.PRODUCT_ID,\n",
    "c1.CUSTOMER_ID,\n",
    "c1.QUANTITY,\n",
    "SUM(c2.QUANTITY) as ROLLING_QTY\n",
    "\n",
    "FROM CUSTOMER_ORDER c1 INNER JOIN CUSTOMER_ORDER c2\n",
    "ON c1.CUSTOMER_ORDER_ID >= c2.CUSTOMER_ORDER_ID\n",
    "\n",
    "GROUP BY 1, 2, 3, 4\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9f62b9f55d1b7",
   "metadata": {},
   "source": [
    "I can simplify this greatly using an `ORDER BY` clause in an analytic function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0007bcf436665",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT CUSTOMER_ORDER_ID, \n",
    "ORDER_DATE,\n",
    "PRODUCT_ID,\n",
    "CUSTOMER_ID,\n",
    "QUANTITY,\n",
    "SUM(QUANTITY) OVER (ORDER BY CUSTOMER_ORDER_ID) as ROLLING_QTY\n",
    "\n",
    "FROM CUSTOMER_ORDER\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178709a5c9834d74",
   "metadata": {},
   "source": [
    "No more complicated self joins with weird `GROUP BY` logic! Now notice that if we did `ORDER BY ORDER_DATE` rather than `ORDER BY CUSTOMER_ORDER_ID` something weird happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380d609bd2e1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT CUSTOMER_ORDER_ID, \n",
    "ORDER_DATE,\n",
    "PRODUCT_ID,\n",
    "CUSTOMER_ID,\n",
    "QUANTITY,\n",
    "SUM(QUANTITY) OVER (ORDER BY ORDER_DATE) as ROLLING_QTY\n",
    "\n",
    "FROM CUSTOMER_ORDER\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5e71d79c352d2",
   "metadata": {},
   "source": [
    "Every record with the same `ORDER_DATE` has the same `ROLLING_QTY`. The reason is the `ORDER_DATE` does not have unique values so each `ORDER_DATE` lumps up each day's total. If we wanted to arbitrarily total on a row-by-row basis, it's better to use an ordered unique field like `CUSTOMER_ORDER_ID`. But if you still want to do the former, use the `ROWS BETWEEN` keyword and specify the range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8af50e808b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT CUSTOMER_ORDER_ID, \n",
    "ORDER_DATE,\n",
    "PRODUCT_ID,\n",
    "CUSTOMER_ID,\n",
    "QUANTITY,\n",
    "SUM(QUANTITY) OVER (ORDER BY ORDER_DATE ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as ROLLING_QTY\n",
    "FROM CUSTOMER_ORDER\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e646f5d40dda718",
   "metadata": {},
   "source": [
    "Be careful when using `ROWS BETWEEN`, as the ordering of the records is arbitrary feeding into the function, and if you re-sort the records you will get confusing results. The default behavior `RANGE BETWEEN` is usually preferred, which works on logical values rather than the individual rows. \n",
    " \n",
    "We can also create rolling averages by changing the bounds. Below we create a rolling average between the 3 preceding and 3 following records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85960d04d66bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT CUSTOMER_ORDER_ID, \n",
    "ORDER_DATE,\n",
    "PRODUCT_ID,\n",
    "CUSTOMER_ID,\n",
    "QUANTITY,\n",
    "AVG(QUANTITY) OVER (ORDER BY ORDER_DATE ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING) as ROLLING_AVG\n",
    "FROM CUSTOMER_ORDER\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a04dc0780f466f",
   "metadata": {},
   "source": [
    " Let's go back to using the default `RANGE BETWEEN` logic. If you want to silo each record to get the rolling total but only within records sharing the `PRODUCT_ID` and `CUSTOMER_ID`, add that `PARTITION BY` again. As you scan the records, notice how the rolling totals are only accounting for records sharing the same `CUSTOMER_ID` and `PRODUCT_ID`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03065c07fbee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT CUSTOMER_ORDER_ID, \n",
    "ORDER_DATE,\n",
    "PRODUCT_ID,\n",
    "CUSTOMER_ID,\n",
    "QUANTITY,\n",
    "SUM(QUANTITY) OVER (PARTITION BY PRODUCT_ID, CUSTOMER_ID ORDER BY ORDER_DATE) as ROLLING_QTY\n",
    "\n",
    "FROM CUSTOMER_ORDER\n",
    "\n",
    "ORDER BY CUSTOMER_ORDER_ID\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60676b08efcde580",
   "metadata": {},
   "source": [
    "## LEAD and LAG "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faaf824df692c52",
   "metadata": {},
   "source": [
    "Two other highly useful windowing functions are `LEAD()` and `LAG()`. These allow you to retrieve another record's value based on an ordered field. Below, we use `LAG()` to look up the previous record's value. Compare the `QUANTITY` and `PREV_QTY` columns below and you will see a pattern! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca47d5954afbdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT CUSTOMER_ORDER_ID, \n",
    "CUSTOMER_ID,\n",
    "ORDER_DATE, \n",
    "PRODUCT_ID,\n",
    "QUANTITY,\n",
    "LAG(QUANTITY, 1, 0) OVER (ORDER BY ORDER_DATE) AS PREV_QTY\n",
    "FROM CUSTOMER_ORDER \n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95b84bf9e188ef",
   "metadata": {},
   "source": [
    "The `LEAD()` will look at the next record ahead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8380dac84d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT CUSTOMER_ORDER_ID, \n",
    "CUSTOMER_ID,\n",
    "ORDER_DATE, \n",
    "PRODUCT_ID,\n",
    "QUANTITY,\n",
    "LEAD(QUANTITY, 1, 0) OVER (ORDER BY ORDER_DATE) AS NEXT_QTY\n",
    "FROM CUSTOMER_ORDER \n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216935f4d292d91",
   "metadata": {},
   "source": [
    "You will see that the second and third arguments, 1 and 0 in these cases, will control the number of records to look-ahead/look-behind and the default value. Below, we change the `LAG()` to retrieve the third record behind it and default the value to `-1` if there is none to retrieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d9fc279d5e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT CUSTOMER_ORDER_ID, \n",
    "CUSTOMER_ID,\n",
    "ORDER_DATE, \n",
    "PRODUCT_ID,\n",
    "QUANTITY,\n",
    "LAG(QUANTITY, 3, -1) OVER (ORDER BY ORDER_DATE) AS PREV_QTY\n",
    "FROM CUSTOMER_ORDER \n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b93ec6a5f7b08",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4bb2331590f3b",
   "metadata": {},
   "source": [
    "The `ROW_NUMBER()` function can be highly helpful with windowing functions to rank items. For example, say I wanted to get the top 3 selling products by customer. I can use `ROW_NUMBER()` to assign a ranking number to each sorted quantity by `CUSTOMER_ID` and `PRODUCT_ID`. Then I can filter for only the first three items.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b917bffe1311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "WITH TOTAL_QTYS AS (\n",
    "  SELECT CUSTOMER_ID, PRODUCT_ID, SUM(QUANTITY) AS TOTAL_QTY \n",
    "  FROM CUSTOMER_ORDER \n",
    "  GROUP BY 1,2\n",
    "),\n",
    "\n",
    "PRODUCT_SALES_BY_CUSTOMER AS (\n",
    "   SELECT CUSTOMER_ID, PRODUCT_ID, TOTAL_QTY,\n",
    "   ROW_NUMBER() OVER (PARTITION BY CUSTOMER_ID ORDER BY TOTAL_QTY DESC) AS RANKING\n",
    "   FROM TOTAL_QTYS\n",
    ") \n",
    "SELECT * FROM PRODUCT_SALES_BY_CUSTOMER \n",
    "WHERE RANKING <= 3\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f802bd096eafc6",
   "metadata": {},
   "source": [
    "`RANK()` and `DENSE_RANK()` are identical to `ROW_NUMBER()` in behavior, except in how identical values are handled. If you want identical values to receive the same ranking, use the `RANK()` function instead of `ROW_NUMBER()`. Use `DENSE_RANK()` if you want to force the values to be consecutive rather than dupes causing ranks to be skipped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6df451f71f05f",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ab956172c7cfb",
   "metadata": {},
   "source": [
    "For the date range of `2024-02-01` to `2024-02-28`, bring in the rolling maximum quantity ordered (up to each `ORDER_DATE`) by `CUSTOMER_ID` and `PRODUCT_ID`. The boilerplate is provided, just replace the question mark `?` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cbf51f622a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT CUSTOMER_ORDER_ID,\n",
    "ORDER_DATE,\n",
    "CUSTOMER_ID,\n",
    "PRODUCT_ID,\n",
    "QUANTITY,\n",
    "? as rolling_max_qty_for_customer_and_product\n",
    "\n",
    "FROM ?\n",
    "WHERE ORDER_DATE BETWEEN '2024-02-01' AND '2024-02-28'\n",
    "\n",
    "ORDER BY CUSTOMER_ORDER_ID\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25102e46095e52e",
   "metadata": {},
   "source": [
    "\n",
    "### SCROLL DOWN FOR ANSWER\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a58732f4ded54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT CUSTOMER_ORDER_ID,\n",
    "ORDER_DATE,\n",
    "CUSTOMER_ID,\n",
    "PRODUCT_ID,\n",
    "QUANTITY,\n",
    "MAX(QUANTITY) OVER(PARTITION BY CUSTOMER_ID, PRODUCT_ID ORDER BY ORDER_DATE) as rolling_max_qty_for_customer_and_product\n",
    "\n",
    "FROM CUSTOMER_ORDER\n",
    "WHERE ORDER_DATE BETWEEN '2024-02-01' AND '2024-02-28'\n",
    "\n",
    "ORDER BY CUSTOMER_ORDER_ID\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
