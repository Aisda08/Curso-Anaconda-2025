{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc921e6fc1ce05fa",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f71f0c21ce973d",
   "metadata": {},
   "source": [
    "While people often associate data cleaning with Python data libraries like pandas, SQL can actually do quite a bit and rise to the challenge. We will cover a few essential functions for data cleaning including the handling of null values, conditional logic, and string operations. \n",
    "\n",
    "For this section, let's practice with the [FAA bird strike dataset](https://wildlife.faa.gov/home) which is a pretty messy dataset with some cleaning opportunities. If you want to learn more about this dataset, I have developed a whole [exploratory data analysis course on Anaconda](https://learning.anaconda.cloud/exploratory-data-analysis-eda-with-python) with it, albeit using pandas. \n",
    "\n",
    "Let's import the database I've prepared and converted into SQLite, for records only since the year 2015. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a4fa2499ccbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd \n",
    "\n",
    "conn = sqlite3.connect('bird_strike.db')\n",
    "\n",
    "pd.read_sql(\"SELECT * FROM BIRD_STRIKE_FAA\", conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e540adcc59bdc292",
   "metadata": {},
   "source": [
    "Let's talk first about case expressions and then null values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108a700592e6e59",
   "metadata": {},
   "source": [
    "## Case Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b8c75823ce534",
   "metadata": {},
   "source": [
    "Let's say I select these fields from the `BIRD_STRIKE_FAA` table. I want to flag any records that involve a Boeing 737, which has several variants. Let's use a wildcard `LIKE` operator to achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f59efa2421f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\" SELECT \n",
    "AIRPORT_ID,\n",
    "AIRPORT,\n",
    "AIRCRAFT\n",
    "\n",
    "FROM BIRD_STRIKE_FAA\n",
    "WHERE AIRCRAFT LIKE '%737%'\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1a19ff5c26aa3",
   "metadata": {},
   "source": [
    "If we wanted to check what values are being captured with `%737%` we can use a `DISTINCT` operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66967fe3527c7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\" \n",
    "SELECT DISTINCT AIRCRAFT\n",
    "FROM BIRD_STRIKE_FAA\n",
    "WHERE AIRCRAFT LIKE '%737%'\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e069cc035dba2f6",
   "metadata": {},
   "source": [
    "Now I want to compare the Boeing 737 incidents against the Airbus A320 incidents. We can categorize them like this using a `CASE` expression, and use the `ELSE` branch to flag everything else as `OTHER`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b52751d4f28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\" \n",
    "SELECT INDEX_NR, \n",
    "AIRCRAFT, \n",
    "INCIDENT_DATE,\n",
    "STATE, \n",
    "AIRPORT_ID, \n",
    "\n",
    "CASE \n",
    "    WHEN AIRCRAFT LIKE '%737%' THEN 'Boeing 737'\n",
    "    WHEN AIRCRAFT LIKE 'A-320%' THEN 'Airbus A-320'\n",
    "    ELSE 'Other'\n",
    "END AIRCRAFT_OF_INTEREST_FLAG \n",
    "\n",
    "FROM BIRD_STRIKE_FAA\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378949231f4fc035",
   "metadata": {},
   "source": [
    "To get an idea of how many records have gotten each of these flags, let's use a `COUNT(*)` paired with a `GROUP BY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a260b1edf9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\" \n",
    "SELECT \n",
    "\n",
    "CASE \n",
    "    WHEN AIRCRAFT LIKE '%737%' THEN 'Boeing 737-related'\n",
    "    WHEN AIRCRAFT LIKE 'A-320%' THEN 'Airbus A-320-related'\n",
    "    ELSE 'Other'\n",
    "END AIRCRAFT_OF_INTEREST_FLAG, \n",
    "\n",
    "COUNT(*) AS RECORD_COUNT\n",
    "\n",
    "FROM BIRD_STRIKE_FAA\n",
    "\n",
    "GROUP BY AIRCRAFT_OF_INTEREST_FLAG\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21c3817e070f7c",
   "metadata": {},
   "source": [
    "The `CASE` expression is useful for all kinds of data transformations and cleanup. Let's talk about null values next. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d577a044676b8b1",
   "metadata": {},
   "source": [
    "## Identifying Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76cde934cb8dff",
   "metadata": {},
   "source": [
    "Let's look at a few fields from the table. Notice how the `RUNWAY` and `PHASE_OF_FLIGHT` fields has some `None` values and the `HEIGHT`, `SPEED`, and `DISTANCE` columns have `NaN`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed1d26cc718ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT AIRPORT, RUNWAY, PHASE_OF_FLIGHT, HEIGHT, SPEED, DISTANCE \n",
    "FROM BIRD_STRIKE_FAA\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11e8e5dea39d5b",
   "metadata": {},
   "source": [
    "pandas is actually turning `NULL` values from the SQLite database into `None` and `NaN` values, depending on the datatype of the column (numeric values will be `NaN`). Nulls can be inconvenient if you do not account for them. Aggregate functions like `SUM`, `MIN`, `MAX`, `COUNT` and `AVG` will ignore null values, which we can leverage later. When you write a `WHERE` condition against a field, it will always ignore null values unless you explicitly handle them. Use the `IS NULL` and `IS NOT NULL` operators to qualify/disqualify null values. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecfdda21f94572",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT AIRPORT, RUNWAY, PHASE_OF_FLIGHT, HEIGHT, SPEED, DISTANCE \n",
    "FROM BIRD_STRIKE_FAA\n",
    "WHERE PHASE_OF_FLIGHT IS NULL\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eeae926fae8802",
   "metadata": {},
   "source": [
    "For these fields, let's count the number of records and number of non-null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4e6a2881933d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS RECORD_COUNT, \n",
    "COUNT(AIRPORT) AS AIRPORT_VALUES, \n",
    "COUNT(RUNWAY) AS RUNWAY_VALUES, \n",
    "COUNT(PHASE_OF_FLIGHT) AS PHASE_OF_FLIGHT_VALUES, \n",
    "COUNT(HEIGHT) AS HEIGHT_VALUES, \n",
    "COUNT(SPEED) AS SPEED_VALUES, \n",
    "COUNT(DISTANCE) AS DISTANCE_VALUES \n",
    "FROM BIRD_STRIKE_FAA\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54897b7c729f98cd",
   "metadata": {},
   "source": [
    "If we wanted to count the number of `NULL` values (rather than non-`NULL` values), we could subtract those field counts from the record counts, or more elegantly `SUM` the number of true values using `IS NULL`. Since `1` will be yielded for true and `0` for false, we can use this to sum the true values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993f66eb841fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS RECORD_COUNT, \n",
    "SUM(AIRPORT IS NULL) AS AIRPORT_NULLS, \n",
    "SUM(RUNWAY IS NULL) AS RUNWAY_NULLS, \n",
    "SUM(PHASE_OF_FLIGHT IS NULL) AS PHASE_OF_FLIGHT_NULLS, \n",
    "SUM(HEIGHT IS NULL) AS HEIGHT_NULLS, \n",
    "SUM(SPEED IS NULL) AS SPEED_NULLS, \n",
    "SUM(DISTANCE IS NULL) AS DISTANCE_NULLS \n",
    "FROM BIRD_STRIKE_FAA\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2efceca229c4a4",
   "metadata": {},
   "source": [
    "We can also ratio the percentage of nulls in each of these fields. Just make sure to convert the operation to floating point values rather than integers, which can be done by multiplying the expression with `1.0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570595dea67ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT COUNT(*) AS RECORD_COUNT, \n",
    "1.0 * SUM(AIRPORT IS NULL) / COUNT(*) AS AIRPORT_NULL_RATE, \n",
    "1.0 * SUM(RUNWAY IS NULL) / COUNT(*)  AS RUNWAY_NULL_RATE, \n",
    "1.0 * SUM(PHASE_OF_FLIGHT IS NULL) / COUNT(*) AS PHASE_OF_FLIGHT_NULL_RATE, \n",
    "1.0 * SUM(HEIGHT IS NULL) / COUNT(*) AS HEIGHT_NULL_RATE, \n",
    "1.0 * SUM(SPEED IS NULL) / COUNT(*) AS SPEED_NULL_RATE, \n",
    "1.0 * SUM(DISTANCE IS NULL) / COUNT(*) AS DISTANCE_NULL_RATE \n",
    "FROM BIRD_STRIKE_FAA\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbf50f6a0cad2f",
   "metadata": {},
   "source": [
    "\n",
    "If we are interested in bird strikes that happened below 500 feet, have we considered the null values and whether we want to include those? This is why documenting what `null` means for a given field is so important. Perhaps the `HEIGHT` was irrelevant, or could not be measured because an instrument was broken. Or perhaps the pilot hastily filed the report and did not care to jot that information down. Regardless, we need to understand why values can be missing and whether they should be included in a given analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0b073212b07f5",
   "metadata": {},
   "source": [
    "> A common mistake for SQL beginners is they use `= NULL` rather than `IS NULL`. This does not work! Always use the latter. \n",
    "\n",
    "If we want to find records where `HEIGHT` is less than 500 feet, but want to include `NULL` values too, then we need to use an `IS NULL` paired with an `OR` to that condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b62254a19a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT AIRPORT, RUNWAY, PHASE_OF_FLIGHT, HEIGHT, SPEED, DISTANCE \n",
    "FROM BIRD_STRIKE_FAA\n",
    "\n",
    "WHERE HEIGHT IS NULL OR HEIGHT < 500\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f4f224ba36100",
   "metadata": {},
   "source": [
    "We could also use a `CASE` expression to turn `NULL` height values into `0`, but there is something even better: the `COALESCE` function. It will take a possibly null value and swap it with a different value if it's null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7c0f26748ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT AIRPORT, RUNWAY, PHASE_OF_FLIGHT, HEIGHT, SPEED, DISTANCE \n",
    "FROM BIRD_STRIKE_FAA\n",
    "\n",
    "WHERE COALESCE(HEIGHT, 0) < 500\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac11bfe706c8392",
   "metadata": {},
   "source": [
    "This coalesce achieves the same thing as a `CASE` expression converting null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2312b4453f5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT AIRPORT, RUNWAY, PHASE_OF_FLIGHT, HEIGHT, SPEED, DISTANCE \n",
    "FROM BIRD_STRIKE_FAA\n",
    "\n",
    "WHERE (CASE WHEN HEIGHT IS NULL THEN 0 ELSE HEIGHT END) < 500\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a58687c7ee4f11",
   "metadata": {},
   "source": [
    "This gives you all the tools you need to filter and handle nulls. Later we will use techniques to impute for missing values with subqueries. There is no once size fits all approach to handling nulls. It will always depend on the task and what is the most appropriate way to handle them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a7444a6078fb0",
   "metadata": {},
   "source": [
    "## String Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e8f8f705ad4ab",
   "metadata": {},
   "source": [
    "A lot of data cleaning will involve string operations. We already saw how to use wildcards with the `LIKE` operator, but there are many functions (and a few operators) targeting strings. \n",
    "\n",
    "| Name      | Description                                                                       |\n",
    "|-----------|-----------------------------------------------------------------------------------|\n",
    "| LENGTH    | Counts the number of characters in a string.                                      |\n",
    "| UPPER     | Converts a string to uppercase.                                                   |\n",
    "| LOWER     | Converts a string to lowercase.                                                   |\n",
    "| SUBSTR    | Extracts a substring with a predefined length at a specific position.             |\n",
    "| TRIM      | Removes specified characters (default space) from the start and end of the string. |\n",
    "| LTRIM     | Removes specified characters (default space) from the start of the string.        |\n",
    "| RTRIM     | Removes specified characters (default space) from the end of the string. .        |\n",
    "| REPLACE   | Replaces matching substrings in a string with another substring.                  |\n",
    "| INSTR     | Returns the position of the first occurrence of the substring, -1 if not found.   |                 \n",
    "| \\|\\|      | Concatenates two or more values into a string |\n",
    "| CONCAT_WS | Concatenates multiple strings into a one string with a separator.                 |\n",
    "| REGEXP    | Determines if a string matches a regular expression.                              | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970fadaf794a463",
   "metadata": {},
   "source": [
    "Let's say we wanted to verify if all `AIRPORT_ID` values are four characters. We can use the `LENGTH()` function to do this and sure enough,  there are a handful that are not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018631f6f10d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT * FROM BIRD_STRIKE_FAA\n",
    "WHERE LENGTH(AIRPORT_ID) != 4\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e7aa906f40fc5",
   "metadata": {},
   "source": [
    "Here is another example where we replace \"ARPT\" with \"AIRPORT\" in the `AIRPORT` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2063fc2c9d9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT \n",
    "AIRPORT,\n",
    "REPLACE(AIRPORT, 'ARPT','AIRPORT') AS AIRPORT_NEW \n",
    "FROM BIRD_STRIKE_FAA\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ca84e2d0578a1",
   "metadata": {},
   "source": [
    "Regular expressions are definitely something you will want to learn when cleaning data, and they are supported in SQL, pandas, Python, and many other platforms. We actually have an [Anaconda course teaching them in Python](https://learning.anaconda.cloud/regular-expressions-in-python). Other SQL platforms will support regular expressions, but SQLite needs it enabled in Python. We can achieve this by running the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789d2ce81260e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def regexp(expr, item):\n",
    "    reg = re.compile(expr)\n",
    "    return reg.search(item) is not None\n",
    "\n",
    "conn.create_function(\"REGEXP\", 2, regexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c613ba398dd072",
   "metadata": {},
   "source": [
    "Now I can look for both Airbus A-320 and A-321 aircraft using a single regular expression as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070b747678e7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\" \n",
    "SELECT * FROM BIRD_STRIKE_FAA\n",
    "WHERE AIRCRAFT REGEXP 'A-32[01]'\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97c0162b6a1321",
   "metadata": {},
   "source": [
    "Let's put several of these string operations together for a practical task. Let's take the `INCIDENT_DATE` and `TIME` columns and merge them together into a proper `DATETIME`. We can remove that \"0 days \" boilerplate on the `TIME` using `SUBSTR`. If the `TIME` value is missing altogether we can use `coalesce()` to replace the null values and make it \"00:00:00\". Finally we concatenate that with the `INCIDENT_DATE` and cast it all as a `DATETIME`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f698dde128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT OPERATOR, \n",
    "AIRCRAFT,\n",
    "AIRPORT,\n",
    "DATETIME(INCIDENT_DATE || ' ' || COALESCE(SUBSTR(TIME, 7), '00:00:00')) AS INCIDENT_DATETIME \n",
    "\n",
    "FROM BIRD_STRIKE_FAA\n",
    "\n",
    "ORDER BY INCIDENT_DATE DESC\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d849f6beb1482e7",
   "metadata": {},
   "source": [
    "> **Should I write these changes back to the table?** \n",
    "> \n",
    "> One of the luxuries of SQL is you can easily take a raw data source and make transformations with SQL queries. You might wonder if these changes should be written back into the table. I would only do this in the context that you are doing extract-transform-load (ETL) work and providing a cleaned dataset for others. You can do this easily by calling `CREATE TABLE` with a `SELECT` to make a new table off a `SELECT` query, or using an `INSERT` with a `SELECT` for an existing target table. But you can maintain all your cleanup work in queries that are ran as needed, without having to store the cleaned data back into a table. Consider even sharing the `SELECT` queries themselves, which can simply be stored in a text file, email, or a view. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25a090e2ea5407",
   "metadata": {},
   "source": [
    "## UNION, UNION ALL, and CASE Tricks  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943041cb7754cb6",
   "metadata": {},
   "source": [
    "Let's say I am interested in comparing the cost of repairs by year for incidents below 1000 feet and above 1000 feet. Most people would use a `UNION ALL` to append these two queries together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcde62cb9ca23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT INCIDENT_YEAR, \n",
    "'BELOW 500' AS THRESHOLD, \n",
    "SUM(COST_REPAIRS) AS TOTAL_REPAIRS\n",
    "FROM BIRD_STRIKE_FAA\n",
    "WHERE HEIGHT < 1000 \n",
    "GROUP BY INCIDENT_YEAR, THRESHOLD\n",
    "\n",
    "UNION ALL \n",
    "\n",
    "SELECT INCIDENT_YEAR, \n",
    "'ABOVE 500' AS THRESHOLD, \n",
    "SUM(COST_REPAIRS) AS TOTAL_REPAIRS\n",
    "FROM BIRD_STRIKE_FAA\n",
    "WHERE HEIGHT >= 1000 \n",
    "GROUP BY INCIDENT_YEAR, THRESHOLD\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6da7f1688ccbd4",
   "metadata": {},
   "source": [
    "This demonstrates the `UNION ALL` which appends the results of both queries together. The `UNION`, which we have not demonstrated, would do the same thing but rid duplicate records. \n",
    "\n",
    "I am not a fan of this use case however, as common as it is. The queries are redundant and therefore have to do two scans of the table, which is inefficient. The only difference between the queries is the `WHERE` condition. If we move that `WHERE` condition into a `CASE` expression, we can consolidate into one query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f4debe8cf4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT INCIDENT_YEAR, \n",
    "CASE WHEN HEIGHT < 1000 THEN 'BELOW 1000' ELSE 'ABOVE 1000' END AS THRESHOLD, \n",
    "SUM(COST_REPAIRS) AS TOTAL_REPAIRS\n",
    "FROM BIRD_STRIKE_FAA\n",
    "GROUP BY INCIDENT_YEAR, THRESHOLD\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45f10630b57c49",
   "metadata": {},
   "source": [
    "We can probably do something even better here. Let's make the `TOTAL_REPAIRS` into two columns, one for the \"ABOVE 1000\" threshold and another for the \"BELOW 1000\" threshold. We can do this with two `CASE` expressions inside the `SUM` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a41ae2dcb0e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT INCIDENT_YEAR, \n",
    "SUM(CASE WHEN HEIGHT < 1000 THEN COST_REPAIRS ELSE NULL END) AS BELOW_1000_COST_REPAIRS, \n",
    "SUM(CASE WHEN HEIGHT >= 1000 THEN COST_REPAIRS ELSE NULL END) AS ABOVE_1000_COST_REPAIRS, \n",
    "SUM(COST_REPAIRS) AS TOTAL_REPAIRS\n",
    "FROM BIRD_STRIKE_FAA\n",
    "GROUP BY INCIDENT_YEAR\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540aef90e6da2215",
   "metadata": {},
   "source": [
    "Because `NULL` values are ignored by `SUM()` and other aggregate functions, we can use that to conditionally ignore values we do not want to count towards the sum. This effectively allows each `SUM()` function (or any aggregate function) to have different `WHERE` conditions. \n",
    "\n",
    "As you can see, the `UNION` and `UNION ALL` are not always a good idea to use and there are often better ways to achieve tasks they are commonly used for, often involving a `CASE` expression. The only use cases they uniquely are qualified for is when you have multiple queries pulling from different tables (not the same one), and transformed into the same structural output to be appended. \n",
    "\n",
    "Use the `CASE` expression to enable powerful patterns that few know about! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8694875547fc147",
   "metadata": {},
   "source": [
    "## Exercise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db40c5d7e857c08",
   "metadata": {},
   "source": [
    "Find the total cost of repairs by `INCIDENT_YEAR` and `INCIDENT_MONTH`, but broken up into two totals: where `SPEED` is less than 200 and `SPEED` is greater than or equal to `200`.  Complete the code below by replacing the question marks `?`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeaedf993490d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT ?, \n",
    "?,\n",
    "? AS BELOW_200_KNOTS_COST_REPAIRS, \n",
    "? AS ABOVE_200_KNOTS_COST_REPAIRS, \n",
    "SUM(COST_REPAIRS) AS TOTAL_REPAIRS\n",
    "FROM BIRD_STRIKE_FAA\n",
    "GROUP BY INCIDENT_YEAR, INCIDENT_MONTH \n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77475528aeed4055",
   "metadata": {},
   "source": [
    "\n",
    "### SCROLL DOWN FOR ANSWER\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6382d175c7d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT INCIDENT_YEAR, \n",
    "INCIDENT_MONTH,\n",
    "SUM(CASE WHEN SPEED < 200 THEN COST_REPAIRS ELSE NULL END) AS BELOW_200_KNOTS_COST_REPAIRS, \n",
    "SUM(CASE WHEN SPEED >= 200 THEN COST_REPAIRS ELSE NULL END) AS ABOVE_200_KNOTS_COST_REPAIRS, \n",
    "SUM(COST_REPAIRS) AS TOTAL_REPAIRS\n",
    "FROM BIRD_STRIKE_FAA\n",
    "GROUP BY 1, 2\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(sql, conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
